{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of several metrics that measure distribution divergence\n",
    "\n",
    "**Important!** We measure divergence of multiple distributions & the distributions are multi-variate!!\n",
    "\n",
    "* Fit a **Logistic/Linear Regression** model and check how well we could fit it.\n",
    "* **Separability index**: for each point, take its nearest neighbors and check if the classes match\n",
    "* **Class scatter matrices/measure**\n",
    "* **Direct class separability measure, DCSM**\n",
    "* **Bhattacharya distance **\n",
    "* **Bin the distributions and measure Collective entropy**\n",
    "* **KL & JS Divergence**\n",
    "* **Earth-Movers-Distance (EMD)** of histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[8.94427191,8.94427191,8.94427191],\n",
    " [8.94427191,8.94427191,8.94427191],\n",
    " [8.94427191,8.94427191,8.94427191],\n",
    " [8.94427191,8.94427191,8.94427191],\n",
    " [8.94427191,8.94427191,8.94427191],\n",
    " [8.94427191,8.94427191,8.94427191],\n",
    " [8.94427191,8.94427191,8.94427191],\n",
    " [8.94427191,8.94427191,8.94427191],\n",
    " [8.94427191,8.94427191,8.94427191],\n",
    " [8.94427191,8.94427191,8.94427191],\n",
    " [8.94427191,8.94427191,8.94427191],\n",
    " [8.94427191,8.94427191,8.94427191],\n",
    " [8.94427191,8.94427191,8.94427191],\n",
    " [8.94427191,8.94427191,8.94427191],\n",
    " [8.94427191,8.94427191,8.94427191],\n",
    " [8.94427191,8.94427191,8.94427191],\n",
    " [8.94427191,8.94427191,8.94427191],\n",
    " [8.94427191,8.94427191,8.94427191],\n",
    " [8.94427191,8.94427191,8.94427191],\n",
    " [8.94427191,8.94427191,8.94427191],\n",
    " [8.94427191,8.94427191,8.94427191],\n",
    " [8.94427191,8.94427191,8.94427191],\n",
    " [8.94427191,8.94427191,8.94427191],\n",
    " [8.94427191,8.94427191,8.94427191],\n",
    " [8.94427191,8.94427191,8.94427191],\n",
    " [8.94427191,8.94427191,8.94427191],\n",
    " [8.94427191,8.94427191,8.94427191]]\n",
    "y = [1,0,0,0,1,1,1,1,0,1,1,1,0,0,0,0,1,1,1,1,1,1,0,0,0,1,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([8.94427191, 8.94427191, 8.94427191]), array([8.94427191, 8.94427191, 8.94427191])]\n",
      "--> [0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "def bhattacharyya(X, y, cells_per_dim=10):\n",
    "    # Calculate lower and upper bound for each dimension\n",
    "    bounds = {}\n",
    "    widths = {}\n",
    "    col_to_del = []\n",
    "    cntr = 0\n",
    "    for d in range(X.shape[1]):\n",
    "        mi, ma = min(X[:, d]), max(X[:, d])\n",
    "        if mi == ma:\n",
    "            col_to_del.append(d)\n",
    "        else:\n",
    "            bounds[cntr] = (mi, ma)\n",
    "            widths[cntr] = (ma - mi) / cells_per_dim\n",
    "            cntr += 1\n",
    "    \n",
    "    X = np.delete(X, col_to_del, axis=1)\n",
    "\n",
    "    if X.shape[1] == 0:\n",
    "        return 1\n",
    "            \n",
    "    # For each datapoint, calculate its cell in the hypercube\n",
    "    cell_assignment_counts = []\n",
    "    label_to_idx = {}\n",
    "    for i, l in enumerate(set(y)): \n",
    "        cell_assignment_counts.append(defaultdict(int))\n",
    "        label_to_idx[l] = i\n",
    "        \n",
    "    unique_assignments = set()\n",
    "    for point_idx, l in zip(range(X.shape[0]), y):\n",
    "        assignment = []\n",
    "        for dim_idx in range(X.shape[1]):\n",
    "            val = X[point_idx, dim_idx]\n",
    "            cell = (val - bounds[dim_idx][0])//widths[dim_idx]\n",
    "            assignment.append(cell)\n",
    "        cell_assignment_counts[label_to_idx[l]][tuple(assignment)] += 1\n",
    "        unique_assignments.add(tuple(assignment))\n",
    "        \n",
    "    totals = {}\n",
    "    for l in set(y):\n",
    "        totals[l] = sum(cell_assignment_counts[label_to_idx[l]].values())\n",
    "    \n",
    "    print(cell_assignment_counts)\n",
    "    \n",
    "    dist = 0\n",
    "    for assign in unique_assignments:\n",
    "        temp = 1\n",
    "        for l in set(y):\n",
    "            temp *= cell_assignment_counts[label_to_idx[l]][assign] / totals[l]\n",
    "        dist += (temp * (temp != 1)) ** (1 / len(totals))\n",
    "    \n",
    "    return dist\n",
    "            \n",
    "    \n",
    "from scipy.spatial.distance import pdist, euclidean\n",
    "def davies_bouldin(X, labels):\n",
    "    n_cluster = len(np.bincount(labels))\n",
    "    cluster_k = [X[labels == k] for k in range(n_cluster)]\n",
    "    centroids = [np.mean(k, axis = 0) for k in cluster_k]\n",
    "    variances = [np.mean([euclidean(p, centroids[i]) for p in k]) for i, k in enumerate(cluster_k)]\n",
    "    db = []\n",
    "\n",
    "    for i in range(n_cluster):\n",
    "        for j in range(n_cluster):\n",
    "            if j != i:\n",
    "                d = euclidean(centroids[i], centroids[j])\n",
    "                if d == 0:\n",
    "                    db.append(0)\n",
    "                else:\n",
    "                    db.append((variances[i] + variances[j]) / euclidean(centroids[i], centroids[j]))\n",
    "                    \n",
    "    return(np.max(db) / n_cluster)\n",
    "    \n",
    "#bhattacharyya(np.array(X), np.array(y))\n",
    "davies_bouldin(np.array(X), np.array(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def partition_feature_vectors(X, y):\n",
    "    distributions = defaultdict(list)\n",
    "    for feature_vector, label in zip(X, y):\n",
    "        distributions[label].append(feature_vector)\n",
    "    return distributions.values()\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "def linear_regression(X, y):\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X, y)\n",
    "    return lr.score(X, y)\n",
    "\n",
    "from sklearn.neighbors import KDTree, BallTree\n",
    "def separability_index(X, y):\n",
    "    kdt = BallTree(X, metric='euclidean')\n",
    "    nearest_neighbors = kdt.query(X, k=2, return_distance=False)[:, 1]\n",
    "    matches = 0\n",
    "    for i in range(len(X)):\n",
    "        matches += (y[i] == y[nearest_neighbors[i]])\n",
    "    return matches / len(X)\n",
    "\n",
    "def class_scatter_matrix(X, y):\n",
    "    # SOURCE 1: https://datascience.stackexchange.com/questions/11554/varying-results-when-calculating-scatter-matrices-for-lda\n",
    "    # SOURCE 2: https://open.uct.ac.za/bitstream/item/17644/Mthembu_Conference_2004.pdf?sequence=1\n",
    "    \n",
    "    # A 10000x2000 dataframe takes LR 35s, this method takes 2s\n",
    "    # --> 2000 columns (which is the bottleneck) is definitely a LARGE upper bound for the shapelet extraction usecase\n",
    "    \n",
    "    # 2500x2000 dataframe takes LR 5.5s and this method 1s\n",
    "    \n",
    "    # Construct a mean vector per class\n",
    "    mean_vecs = {}\n",
    "    for label in set(y):\n",
    "        mean_vecs[label] = np.mean(X[y==label], axis=0)\n",
    "        \n",
    "    # Construct the within class matrix (S_w)\n",
    "    d = X.shape[1]\n",
    "    S_w = np.zeros((d, d))\n",
    "    for label, mv in zip(set(y), mean_vecs):\n",
    "        class_scatter = np.cov(X[y==label].T)\n",
    "        S_w += class_scatter\n",
    "        \n",
    "    # Construct an overall mean vector\n",
    "    mean_overall = np.mean(X, axis=0)\n",
    "    \n",
    "    # Construct the between class matrix (S_b)\n",
    "    S_b = np.zeros((d, d))\n",
    "    for i in mean_vecs:\n",
    "        mean_vec = mean_vecs[i]\n",
    "        n = X[y==i, :].shape[0]\n",
    "        mean_vec = mean_vec.reshape(d, 1)\n",
    "        mean_overall = mean_overall.reshape(d, 1)\n",
    "        S_b += n * (mean_vec - mean_overall).dot((mean_vec - mean_overall).T)\n",
    "        \n",
    "    # Differs a bit from proposed formule (where we divide by trace(S_w)), we make sure it is bounded by 1.0!\n",
    "    return np.trace(S_b) / np.trace(S_w + S_b)\n",
    "\n",
    "def bhattacharya_distance(X, y):\n",
    "    # TODO: Rewrite this code --> Iterate over each datapoint and calculate the according cell to solve curse of dim\n",
    "    # http://www.eecs.yorku.ca/research/techreports/2015/EECS-2015-02.pdf\n",
    "    histograms = {}\n",
    "    for c in set(y):\n",
    "        # This crashes!!!!\n",
    "        histograms[c] = np.histogramdd(X[:, :20], normed=True)[0].flatten()\n",
    "        \n",
    "    d = 0\n",
    "    for i in range(len(histograms[y[0]])):\n",
    "        s = 1\n",
    "        for c in histograms:\n",
    "            s *= histograms[c][i]\n",
    "        d += s ** (1/len(histograms))\n",
    "            \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Samples = 100 || #Features = 20 || Separation = 0.05\n",
      "LR score = 0.54 || time = 0.003136873245239258\n",
      "SI score = 0.4925367823724272 || time = 0.0015692710876464844\n",
      "#Samples = 100 || #Features = 20 || Separation = 0.25\n",
      "LR score = 0.55 || time = 0.00263214111328125\n",
      "SI score = 0.4426408198846103 || time = 0.0010442733764648438\n",
      "#Samples = 100 || #Features = 20 || Separation = 0.5\n",
      "LR score = 0.59 || time = 0.0024421215057373047\n",
      "SI score = 0.414602055494247 || time = 0.0010521411895751953\n",
      "#Samples = 100 || #Features = 20 || Separation = 1.0\n",
      "LR score = 0.76 || time = 0.0024251937866210938\n",
      "SI score = 0.7690458416694668 || time = 0.0016279220581054688\n",
      "#Samples = 100 || #Features = 100 || Separation = 0.05\n",
      "LR score = 1.0 || time = 0.012335062026977539\n",
      "SI score = 0.4065327635422319 || time = 0.002017498016357422\n",
      "#Samples = 100 || #Features = 100 || Separation = 0.25\n",
      "LR score = 1.0 || time = 0.010277509689331055\n",
      "SI score = 0.4091861251720159 || time = 0.009413003921508789\n",
      "#Samples = 100 || #Features = 100 || Separation = 0.5\n",
      "LR score = 1.0 || time = 0.009890317916870117\n",
      "SI score = 0.41712264459235276 || time = 0.001766204833984375\n",
      "#Samples = 100 || #Features = 100 || Separation = 1.0\n",
      "LR score = 1.0 || time = 0.008780479431152344\n",
      "SI score = 0.6604654537790151 || time = 0.0034067630767822266\n",
      "#Samples = 100 || #Features = 500 || Separation = 0.05\n",
      "LR score = 1.0 || time = 0.022707462310791016\n",
      "SI score = 0.39873687331414576 || time = 0.023078441619873047\n",
      "#Samples = 100 || #Features = 500 || Separation = 0.25\n",
      "LR score = 1.0 || time = 0.04250216484069824\n",
      "SI score = 0.3935827084249453 || time = 0.021605491638183594\n",
      "#Samples = 100 || #Features = 500 || Separation = 0.5\n",
      "LR score = 1.0 || time = 0.05050468444824219\n",
      "SI score = 0.4360633355540618 || time = 0.02399611473083496\n",
      "#Samples = 100 || #Features = 500 || Separation = 1.0\n",
      "LR score = 1.0 || time = 0.022434711456298828\n",
      "SI score = 0.47405147476080695 || time = 0.01286625862121582\n",
      "#Samples = 100 || #Features = 2000 || Separation = 0.05\n",
      "LR score = 1.0 || time = 0.06918215751647949\n",
      "SI score = 0.40334809701644375 || time = 0.38129162788391113\n",
      "#Samples = 100 || #Features = 2000 || Separation = 0.25\n",
      "LR score = 1.0 || time = 0.09558320045471191\n",
      "SI score = 0.40208288517196367 || time = 0.5131862163543701\n",
      "#Samples = 100 || #Features = 2000 || Separation = 0.5\n",
      "LR score = 1.0 || time = 0.09760904312133789\n",
      "SI score = 0.405310467491337 || time = 0.3216686248779297\n",
      "#Samples = 100 || #Features = 2000 || Separation = 1.0\n",
      "LR score = 1.0 || time = 0.09475374221801758\n",
      "SI score = 0.4038299422543164 || time = 0.32518815994262695\n",
      "#Samples = 1000 || #Features = 20 || Separation = 0.05\n",
      "LR score = 0.457 || time = 0.011197805404663086\n",
      "SI score = 0.5571573298598583 || time = 0.0017604827880859375\n",
      "#Samples = 1000 || #Features = 20 || Separation = 0.25\n",
      "LR score = 0.458 || time = 0.020173072814941406\n",
      "SI score = 0.5439556018620079 || time = 0.0017752647399902344\n",
      "#Samples = 1000 || #Features = 20 || Separation = 0.5\n",
      "LR score = 0.527 || time = 0.007621288299560547\n",
      "SI score = 0.9008554983009286 || time = 0.0016484260559082031\n",
      "#Samples = 1000 || #Features = 20 || Separation = 1.0\n",
      "LR score = 0.723 || time = 0.008847236633300781\n",
      "SI score = 0.971775083633134 || time = 0.0014333724975585938\n",
      "#Samples = 1000 || #Features = 100 || Separation = 0.05\n",
      "LR score = 0.529 || time = 0.032662391662597656\n",
      "SI score = 0.40480749499851804 || time = 0.0034017562866210938\n",
      "#Samples = 1000 || #Features = 100 || Separation = 0.25\n",
      "LR score = 0.528 || time = 0.03887224197387695\n",
      "SI score = 0.5384295147828482 || time = 0.007711172103881836\n",
      "#Samples = 1000 || #Features = 100 || Separation = 0.5\n",
      "LR score = 0.596 || time = 0.10092997550964355\n",
      "SI score = 0.6757605409669358 || time = 0.0039484500885009766\n",
      "#Samples = 1000 || #Features = 100 || Separation = 1.0\n",
      "LR score = 0.623 || time = 0.04968714714050293\n",
      "SI score = 0.7927596059954596 || time = 0.003987312316894531\n",
      "#Samples = 1000 || #Features = 500 || Separation = 0.05\n",
      "LR score = 0.889 || time = 0.7693958282470703\n",
      "SI score = 0.3807650019600271 || time = 0.051862478256225586\n",
      "#Samples = 1000 || #Features = 500 || Separation = 0.25\n",
      "LR score = 0.974 || time = 0.7983453273773193\n",
      "SI score = 0.44773042086905673 || time = 0.027454376220703125\n",
      "#Samples = 1000 || #Features = 500 || Separation = 0.5\n",
      "LR score = 0.995 || time = 0.9489078521728516\n",
      "SI score = 0.5248145419986583 || time = 0.03478813171386719\n",
      "#Samples = 1000 || #Features = 500 || Separation = 1.0\n",
      "LR score = 1.0 || time = 0.7074286937713623\n",
      "SI score = 0.7120209056686462 || time = 0.03957104682922363\n",
      "#Samples = 1000 || #Features = 2000 || Separation = 0.05\n",
      "LR score = 1.0 || time = 1.5175185203552246\n",
      "SI score = 0.39829726621741735 || time = 0.5701539516448975\n",
      "#Samples = 1000 || #Features = 2000 || Separation = 0.25\n",
      "LR score = 1.0 || time = 1.4090077877044678\n",
      "SI score = 0.4166429072342317 || time = 0.4681575298309326\n",
      "#Samples = 1000 || #Features = 2000 || Separation = 0.5\n",
      "LR score = 1.0 || time = 1.501556396484375\n",
      "SI score = 0.4446719118465161 || time = 0.5370945930480957\n",
      "#Samples = 1000 || #Features = 2000 || Separation = 1.0\n",
      "LR score = 1.0 || time = 1.4935872554779053\n",
      "SI score = 0.47070905321068535 || time = 0.5812995433807373\n",
      "#Samples = 2500 || #Features = 20 || Separation = 0.05\n",
      "LR score = 0.3948 || time = 0.04739785194396973\n",
      "SI score = 0.44069051179413293 || time = 0.0023963451385498047\n",
      "#Samples = 2500 || #Features = 20 || Separation = 0.25\n",
      "LR score = 0.4488 || time = 0.024089813232421875\n",
      "SI score = 0.8652093749368499 || time = 0.003326416015625\n",
      "#Samples = 2500 || #Features = 20 || Separation = 0.5\n",
      "LR score = 0.4556 || time = 0.021423816680908203\n",
      "SI score = 0.9642945580606427 || time = 0.0040323734283447266\n",
      "#Samples = 2500 || #Features = 20 || Separation = 1.0\n",
      "LR score = 0.6976 || time = 0.029565811157226562\n",
      "SI score = 0.9848869826184095 || time = 0.003033876419067383\n",
      "#Samples = 2500 || #Features = 100 || Separation = 0.05\n",
      "LR score = 0.4424 || time = 0.11383986473083496\n",
      "SI score = 0.43604362772430744 || time = 0.006978034973144531\n",
      "#Samples = 2500 || #Features = 100 || Separation = 0.25\n",
      "LR score = 0.4716 || time = 0.10988306999206543\n",
      "SI score = 0.6465731642070928 || time = 0.006537914276123047\n",
      "#Samples = 2500 || #Features = 100 || Separation = 0.5\n",
      "LR score = 0.6696 || time = 0.12647223472595215\n",
      "SI score = 0.899845160655619 || time = 0.006946563720703125\n",
      "#Samples = 2500 || #Features = 100 || Separation = 1.0\n",
      "LR score = 0.7596 || time = 0.14833950996398926\n",
      "SI score = 0.9511341381704482 || time = 0.006585836410522461\n",
      "#Samples = 2500 || #Features = 500 || Separation = 0.05\n",
      "LR score = 0.584 || time = 0.5114893913269043\n",
      "SI score = 0.3977303831215449 || time = 0.06667065620422363\n",
      "#Samples = 2500 || #Features = 500 || Separation = 0.25\n",
      "LR score = 0.62 || time = 0.5465762615203857\n",
      "SI score = 0.49596138875692763 || time = 0.0588991641998291\n",
      "#Samples = 2500 || #Features = 500 || Separation = 0.5\n",
      "LR score = 0.6736 || time = 0.6609854698181152\n",
      "SI score = 0.5818160659622899 || time = 0.06453204154968262\n",
      "#Samples = 2500 || #Features = 500 || Separation = 1.0\n",
      "LR score = 0.8104 || time = 0.8017051219940186\n",
      "SI score = 0.852750075549402 || time = 0.06261730194091797\n",
      "#Samples = 2500 || #Features = 2000 || Separation = 0.05\n",
      "LR score = 1.0 || time = 8.902819871902466\n",
      "SI score = 0.4056762752022084 || time = 0.6834695339202881\n",
      "#Samples = 2500 || #Features = 2000 || Separation = 0.25\n",
      "LR score = 1.0 || time = 9.038424968719482\n",
      "SI score = 0.41879600684894724 || time = 0.8650307655334473\n",
      "#Samples = 2500 || #Features = 2000 || Separation = 0.5\n",
      "LR score = 1.0 || time = 7.827504873275757\n",
      "SI score = 0.47045926652235515 || time = 0.6512482166290283\n",
      "#Samples = 2500 || #Features = 2000 || Separation = 1.0\n",
      "LR score = 1.0 || time = 5.610850811004639\n",
      "SI score = 0.7312414564532248 || time = 0.9629623889923096\n",
      "#Samples = 10000 || #Features = 20 || Separation = 0.05\n",
      "LR score = 0.3748 || time = 0.12910056114196777\n",
      "SI score = 0.5405401438069097 || time = 0.010973453521728516\n",
      "#Samples = 10000 || #Features = 20 || Separation = 0.25\n",
      "LR score = 0.4478 || time = 0.14702296257019043\n",
      "SI score = 0.946045201560104 || time = 0.00745081901550293\n",
      "#Samples = 10000 || #Features = 20 || Separation = 0.5\n",
      "LR score = 0.5287 || time = 0.13192033767700195\n",
      "SI score = 0.9854431392260148 || time = 0.01313471794128418\n",
      "#Samples = 10000 || #Features = 20 || Separation = 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR score = 0.613 || time = 0.17855405807495117\n",
      "SI score = 0.9937306187582808 || time = 0.006773233413696289\n",
      "#Samples = 10000 || #Features = 100 || Separation = 0.05\n",
      "LR score = 0.3841 || time = 0.3342397212982178\n",
      "SI score = 0.47580986967018424 || time = 0.023267507553100586\n",
      "#Samples = 10000 || #Features = 100 || Separation = 0.25\n",
      "LR score = 0.4631 || time = 0.32274842262268066\n",
      "SI score = 0.829036975864065 || time = 0.02506875991821289\n",
      "#Samples = 10000 || #Features = 100 || Separation = 0.5\n",
      "LR score = 0.5889 || time = 0.39772629737854004\n",
      "SI score = 0.9449091964766645 || time = 0.025813579559326172\n",
      "#Samples = 10000 || #Features = 100 || Separation = 1.0\n",
      "LR score = 0.6162 || time = 0.49792027473449707\n",
      "SI score = 0.9791902859441074 || time = 0.02372288703918457\n",
      "#Samples = 10000 || #Features = 500 || Separation = 0.05\n",
      "LR score = 0.4577 || time = 1.3018431663513184\n",
      "SI score = 0.420656572106113 || time = 0.2711014747619629\n",
      "#Samples = 10000 || #Features = 500 || Separation = 0.25\n",
      "LR score = 0.4819 || time = 1.4415347576141357\n",
      "SI score = 0.5603877726989915 || time = 0.2911038398742676\n",
      "#Samples = 10000 || #Features = 500 || Separation = 0.5\n",
      "LR score = 0.5324 || time = 1.5390305519104004\n",
      "SI score = 0.837875636051399 || time = 0.25881481170654297\n",
      "#Samples = 10000 || #Features = 500 || Separation = 1.0\n",
      "LR score = 0.611 || time = 1.8676249980926514\n",
      "SI score = 0.9216124958314496 || time = 0.24006938934326172\n",
      "#Samples = 10000 || #Features = 2000 || Separation = 0.05\n",
      "LR score = 0.5971 || time = 8.328259706497192\n",
      "SI score = 0.40489786064719463 || time = 2.2108407020568848\n",
      "#Samples = 10000 || #Features = 2000 || Separation = 0.25\n",
      "LR score = 0.6285 || time = 7.739629745483398\n",
      "SI score = 0.5162228569829876 || time = 2.7544729709625244\n",
      "#Samples = 10000 || #Features = 2000 || Separation = 0.5\n",
      "LR score = 0.673 || time = 8.887169361114502\n",
      "SI score = 0.6496485084004169 || time = 2.4705870151519775\n",
      "#Samples = 10000 || #Features = 2000 || Separation = 1.0\n",
      "LR score = 0.9433 || time = 34.3961877822876\n",
      "SI score = 0.8601782539590819 || time = 2.2036945819854736\n"
     ]
    }
   ],
   "source": [
    "class_seps = [0.05, 0.25, 0.5, 1.0]\n",
    "samples = [100, 1000, 2500, 10000]\n",
    "features = [20, 100, 500, 2000]\n",
    "for nr_samples in samples:\n",
    "    for nr_features in features:\n",
    "        lr_scores = []\n",
    "        csm_scores = []\n",
    "        for sep in class_seps:\n",
    "            X, y = make_classification(n_samples=nr_samples, n_features=nr_features, class_sep=sep, n_classes=3, n_informative=5)\n",
    "            print('#Samples = {} || #Features = {} || Separation = {}'.format(nr_samples, nr_features, sep))\n",
    "            start = time.time()\n",
    "            score = linear_regression(X, y)\n",
    "            end = time.time()\n",
    "            lr_scores.append(score)\n",
    "            print('LR score = {} || time = {}'.format(score, end-start))\n",
    "            start = time.time()\n",
    "            score = class_scatter_matrix(X, y)\n",
    "            if score > 1.0:\n",
    "                print('Not bounded...')\n",
    "            end = time.time()\n",
    "            csm_scores.append(score)\n",
    "            print('SI score = {} || time = {}'.format(score, end-start))\n",
    "        #print(np.corrcoef(lr_scores, class_seps), np.corrcoef(csm_scores, class_seps)[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
